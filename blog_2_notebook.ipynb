{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Crypto Trading Dashboard - The First Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As discussed in my first blog post [here](https://nate23424533.wordpress.com/2022/11/08/my-path-to-data-science/), my initial interest in data science stemmed from my desire to build a tool to download real-time trade history for a cryptocurrency coin so that I could calculate and analyze my own trading statistics in real-time like professional traders. \n",
    "\n",
    "My second blog post, along with this notebook, attempts to the answer the following question: \n",
    "\n",
    "```\n",
    "Based on what I've learned in Phase 1, do I have the ability to build the beginnings of a Crypto Trading Dashboard that obtains, loads, and begins to analyze complete trade history for a crypto coin?***\n",
    "```\n",
    "\n",
    "Click [here](https://nate23424533.wordpress.com/2022/12/05/building-a-crypto-trading-dashboard-the-first-steps/) for the related blog post.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## System / Notebook Preparation\n",
    "### Install necessary external packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install gdown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import exists\n",
    "import gdown\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.dates as mdates\n",
    "%matplotlib inline\n",
    "import requests\n",
    "import datetime\n",
    "import json\n",
    "import time\n",
    "debugging = True\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download historical trade data\n",
    "\n",
    "Historical trade data is in two sources:\n",
    "- The bulk of the historical trade data is hosted on Google drive. Currently this source holds trade data from the beginning of trading of this coin on Kraken (6/17/21) to the end of Q2-2022 (6/30/22). \n",
    "- More recent trade data must be downloaded from the Kraken API.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download and load bulk trade data file from Google drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Make a data directory if it doesn't exist\n",
    "if not os.path.exists(\"data\"):\n",
    "    ! mkdir data\n",
    "\n",
    "zfile = 'data/sol.zip'\n",
    "if not exists(zfile):\n",
    "    # Download the historical trades from Google file repository\n",
    "    url = 'https://drive.google.com/uc?id=1nrVrxbs7MaMHKpCObij9W-Mmq-B9QntK'\n",
    "    gdown.download(url, zfile, quiet=False)\n",
    "\n",
    "zf = zipfile.ZipFile(zfile)\n",
    "# Load the SOL-USD trade data file from the zip file\n",
    "df_bulk = pd.read_csv(zf.open(\"SOLUSD.csv\"), header=None)\n",
    "df_bulk.columns = ['date_time', 'price', 'quantity']\n",
    "print(f\"The bulk trade data file has {len(df_bulk):,} trade records in it.\\n\")\n",
    "df_bulk.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Success\n",
    "We've successfully downloaded and loaded the bulk trade data from Google Drive into a pandas dataframe.  As discussed, the bulk data only has trades up to 6/30/22.  Next, we need to load the more recent trade history.  \n",
    "\n",
    "### Download recent trade data from Kraken API\n",
    "\n",
    "Kraken makes its recent trade history available with its API.  To download this data, you send an http request with a unix formatted date/time value in the URL.  The API responds with the first 1,000 trades from that date/time.  You can use the last date/time of this trade data to then request the next 1000 trades.  Using this method, you can loop through and download all recent trade data since the date of the last trade record you have. \n",
    "\n",
    "Example request:  https://api.kraken.com/0/public/Trades?pair=solusd&since=1624192440629160521\n",
    "\n",
    "Note that Kraken only provides a maximum of 1,000 trades in each API request, and it limits its API responses to ~1 response per second.  This forces us to pause 1 second in between each request or we will get an error.\n",
    "\n",
    "**Warning**\n",
    "\n",
    "Downloading all trade data from the date of the end of the bulk data will take 30-45 min.  However, most of this data has been downloaded and saved in a data file within the data folder named 'data/api_dat.csv' so you don't have run it for long.  Running the following cell will bring current this API data file to the most recent trade. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# constants\n",
    "prefix=\"https://api.kraken.com/0/public/Trades?pair=solusd&since=\"\n",
    "columns = ['price', 'quantity', 'date_time_long', \"other_1\", \"other_2\", \"other_3\", \"trade_no\"]\n",
    "API_pause = 1    # Pause duration for API calls per API documentation\n",
    "\n",
    "# Check to see if there is an api trade file in the data folder\n",
    "if not exists('data/api_data.csv'):\n",
    "    # No file\n",
    "    df_api_new = pd.DataFrame(columns=columns)\n",
    "    df_api_full = df_api_new.copy()\n",
    "    print(\"API data file doesn't exist.  Start downloading from the date of the last record from bulk data...\\n\")\n",
    "    # add microseconds to the last bulk trade trade time\n",
    "    last = str(int(df_bulk['date_time'].max() + 1))\n",
    "    last_date = datetime.datetime.utcfromtimestamp(int(last[0:10]))\n",
    "    print(\"Last trade date in bulk file:           \", last_date, \" (\", last, \")\", sep=\"\")\n",
    "    last = str(int(last))\n",
    "    print(f\"Current API trade list:        {len(df_api_full):>10,} records\\n\")\n",
    "else:\n",
    "    # Has a file - load it\n",
    "    print(\"API data file exists.  Loading what we have, then start downloading the rest...\\n\")\n",
    "    df_api_full = pd.read_csv('data/api_data.csv')\n",
    "    first = str(df_api_full['date_time_long'].min())\n",
    "    first_date = datetime.datetime.utcfromtimestamp(int(first[0:10]))\n",
    "    last = str(df_api_full['date_time_long'].max())\n",
    "    last_split = last.split(sep=\".\")\n",
    "    new_last = last_split[0] + \".\" + str(int(last_split[1].ljust(9, '0'))+1)\n",
    "    last_date = datetime.datetime.utcfromtimestamp(int(last[0:10]))\n",
    "    print(\"First trade date in file:               \", first_date, \" (\", first, \")\", sep=\"\")\n",
    "    print(\"Last trade date in file:                \", last_date, \" (\", last, \")\", sep=\"\")\n",
    "    # print(\"New Last:                       \", new_last)\n",
    "    print(f\"Current API trade list:                {len(df_api_full):>10,} records\\n\\n\", end=\"\")\n",
    "    \n",
    "# Start endless loop (ends when the data is complete/current)\n",
    "while 1:\n",
    "\n",
    "    full_url = prefix + str(last)\n",
    "    \n",
    "    # Call the web request and confirm we received it\n",
    "    print(f\"Sending request:             \\t\\t{last}, {last_date}\")\n",
    "    response = requests.get(full_url)\n",
    "    \n",
    "    # Verify that the request was successful.  \n",
    "    if response.status_code != 200:\n",
    "        print(\"\\n\\nAPI request failed. Ending process...\")\n",
    "        break\n",
    "\n",
    "    # Successful request\n",
    "    response_dict = json.loads(response.text)\n",
    "    if \"result\" not in response_dict.keys():\n",
    "        print(\"Response received:           \\t\\tNo result. Pausing 5 seconds and then continuing...\\n\")\n",
    "        time.sleep(5)\n",
    "        continue\n",
    "    \n",
    "    trades = response_dict['result']['SOLUSD']   #response will have 1 duplicate trade in it.  drop it\n",
    "    new_trade_count = len(trades)\n",
    "    \n",
    "    # if new trade data, add it to our list\n",
    "    if new_trade_count > 0:\n",
    "        # Build a new dataframe with the new trade data, and then concatenate it\n",
    "        df_api_new = pd.DataFrame(trades)\n",
    "        df_api_new.columns = columns\n",
    "        df_api_full = pd.concat([df_api_full, df_api_new], ignore_index=True)\n",
    "        \n",
    "        # Confirm trades added\n",
    "        print(f\"Response received:           \\t\\t{new_trade_count:,} records added\")\n",
    "        print(f\"Current API trade list:      \\t\\t{len(df_api_full):,} records\\n\")\n",
    "\n",
    "        # Get new last\n",
    "        last = str(int(response_dict['result']['last'])+1)\n",
    "        last_date = datetime.datetime.utcfromtimestamp(int(last[0:10]))\n",
    "        time.sleep(API_pause)\n",
    "    else:\n",
    "        print(f\"Response received:           \\t\\t{new_trade_count:,} records added\")\n",
    "        print(\"\\nTrade data is complete. Ending process...\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save downloaded recent trade data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Drop any duplicates that may have occurred (shouldn't be any) \n",
    "df_api_full = df_api_full.drop_duplicates(keep='first')\n",
    "\n",
    "# Save it\n",
    "df_api_full.to_csv('data/api_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load recent trade data from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_api_from_file_1 = pd.read_csv('data/api_data.csv')\n",
    "df_api_from_file_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a copy of the date_time column and cut off the microseconds\n",
    "df_api_from_file_1['date_time'] = df_api_from_file_1['date_time_long'].map(lambda x: int(str(x)[0:10]))\n",
    "\n",
    "# Carve out what we need\n",
    "df_api_from_file_2 = df_api_from_file_1[['price', 'quantity', 'date_time']]\n",
    "df_api_from_file_2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine bulk and recent trade data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine bulk and recent trade data data frames\n",
    "df_combined = pd.concat([df_bulk, df_api_from_file_2], ignore_index=True)\n",
    "df_combined.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add features that can help us create visualizations\n",
    "- Extended price\n",
    "- A new column represented the date_time formated as a datetime object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Make copy of that column and convert to date/time\n",
    "df_combined['nk_date_time'] = df_combined['date_time']\n",
    "df_combined['nk_date_time'] = pd.to_datetime(df_combined['nk_date_time'],unit='s')\n",
    "\n",
    "# Add extended_price\n",
    "df_combined['ext_price'] = df_combined['price'] * df_combined['quantity']\n",
    "\n",
    "# check it out\n",
    "df_combined.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from the cell below that there are a lot of trade records with the same date_time value.  This represents multiple trades performed within the same second.  We want to combine these records for visualizations to be accurate so that each time value only has one record. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined.duplicated(subset=['date_time']).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by time value (seconds)\n",
    "df_combined_g_s = df_combined.groupby('nk_date_time')[['ext_price', 'quantity']].agg(['sum'])\n",
    "\n",
    "df_combined_g_s['avg_price'] = df_combined_g_s['ext_price'] / df_combined_g_s['quantity']\n",
    "df_combined_g_s.reset_index(inplace=True)\n",
    "df_combined_g_s.columns = ['nk_date_time', 'ext_price', 'quantity', 'avg_price']\n",
    "df_combined_g_s.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_combined_g_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Use Pandas Grouping to analyze data using different time intervals**\n",
    "\n",
    "As you can see, grouping by second reduced the number of rows by almost two-third (from 6 million to 2 million), while the trade data is still highly detailed to the time period it relates.  In reality, we aren't at the point of analyzing trade data by seconds yet.  Let's group by days and months to visualize some high level trends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by day\n",
    "df_combined_g_d = df_combined_g_s.groupby(pd.Grouper(key='nk_date_time', freq='d'))[['ext_price', 'quantity']].agg('sum')\n",
    "df_combined_g_d['avg_price'] = df_combined_g_d['ext_price'] / df_combined_g_d['quantity']\n",
    "df_combined_g_d.reset_index(inplace=True)\n",
    "df_combined_g_d.columns = ['nk_date_time', 'ext_price', 'quantity', 'avg_price']\n",
    "\n",
    "# Group by month\n",
    "df_combined_g_m = df_combined_g_s.groupby(pd.Grouper(key='nk_date_time', freq='m'))[['ext_price', 'quantity']].agg('sum')\n",
    "df_combined_g_m['avg_price'] = df_combined_g_m['ext_price'] / df_combined_g_m['quantity']\n",
    "df_combined_g_m.reset_index(inplace=True)\n",
    "df_combined_g_m.columns = ['nk_date_time', 'ext_price', 'quantity', 'avg_price']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the trade data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize daily average price and volume history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "min_trade_date = df_combined_g_d.nk_date_time.min()\n",
    "max_trade_date = df_combined_g_d.nk_date_time.max()\n",
    "\n",
    "sns.set(style=\"darkgrid\")\n",
    "fig, ax = plt.subplots(figsize=(15, 7))\n",
    "\n",
    "new_title = \"Daily Average Price/Volume History: Solana (SOL-USD)\\n\"\n",
    "new_title = new_title + min_trade_date.strftime('%m/%d/%y') + \" - \" + max_trade_date.strftime('%m/%d/%y')\n",
    "ax.set_title(new_title, fontsize=14)\n",
    "\n",
    "x = df_combined_g_d['nk_date_time'] #tr.strftime(\"%m-%y\")\n",
    "y = df_combined_g_d['avg_price']\n",
    "ax.set_ylabel(\"Average price (USD)\", fontsize=14, labelpad=20)\n",
    "ax.set_xlabel(\"Date\", fontsize=14, labelpad=18)\n",
    "ax.tick_params(axis='both', which='major', labelsize=12)\n",
    "ax.tick_params(axis='both', which='minor', labelsize=12)\n",
    "ax.yaxis.set_major_formatter('${x:,.0f}')\n",
    "myFmt = mdates.DateFormatter('%m/%d/%y')\n",
    "ax.xaxis.set_major_formatter(myFmt)\n",
    "sns.lineplot(x=x,y=y,ci=None, ax=ax, alpha=.7, label='Average price');\n",
    "\n",
    "ax2=ax.twinx()\n",
    "x=df_combined_g_d['nk_date_time']\n",
    "y=df_combined_g_d['ext_price']\n",
    "ax2.set_ylabel(\"Trade Volume (USD)\", fontsize=14, labelpad=20)\n",
    "ax2.tick_params(axis='both', which='major', labelsize=12)\n",
    "ax2.tick_params(axis='both', which='minor', labelsize=12)\n",
    "ax2.yaxis.set_major_formatter('${x:,.0f}')\n",
    "sns.lineplot(x=x,y=y,ci=None, ax=ax2, color='green', alpha=.7, label='Volume');\n",
    "\n",
    "ax.legend_.remove()\n",
    "ax2.legend_.remove()\n",
    "fig.legend(loc=\"upper right\", bbox_to_anchor=(1,1), bbox_transform=ax.transAxes);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize trade volume history by month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "min_trade_date = df_combined_g_d.nk_date_time.min()\n",
    "max_trade_date = df_combined_g_d.nk_date_time.max()\n",
    "\n",
    "months = df_combined_g_m['nk_date_time'].tolist()\n",
    "months = [x.strftime('%b-%y') for x in months]\n",
    "\n",
    "sns.set(style=\"darkgrid\")\n",
    "fig, ax = plt.subplots(figsize=(15, 7))\n",
    "sns.barplot(data=df_combined_g_m,x=months,y=\"ext_price\",ci=None, ax=ax);\n",
    "new_title = \"Monthly Total Trade Volume: Solana (SOL-USD)\\n\"\n",
    "new_title = new_title + min_trade_date.strftime('%m/%d/%y') + \" - \" + max_trade_date.strftime('%m/%d/%y')\n",
    "ax.set_title(new_title, fontsize=14)\n",
    "ax.set_ylabel(\"Volume (USD)\", fontsize=14, labelpad=20)\n",
    "ax.set_xlabel(\"Date\", fontsize=14, labelpad=18)\n",
    "ax.tick_params(axis='both', which='major', labelsize=12)\n",
    "ax.tick_params(axis='both', which='minor', labelsize=12)\n",
    "ax.yaxis.set_major_formatter('${x:,.0f}')\n",
    "#myFmt = mdates.DateFormatter('%m/%y')\n",
    "#ax.xaxis.set_major_formatter(myFmt)\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=90);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "### Data Conclusions\n",
    "\n",
    "From the visualizations below, we notice that: \n",
    "\n",
    "- The value of Solana is down significantly from its all time highs of around $250. \n",
    "- Even in the downward trend since November 2021, there are periods where prices spike up - indicating an opportunity for short-term profits. \n",
    "- Trade volume of Solana has also trended down.  \n",
    "- Trade volume tends to spike when the price is changing more quickly, whether the price is going up or down.  This could possibly be used as a differentiator between meaningful and less meaningful price fluctuations. \n",
    "\n",
    "### Blog Conclusions\n",
    "Going back to the question posed in the introduction of this notebook: \n",
    "\n",
    "```\n",
    "Based on what I've learned in Phase 1, do I have the ability to build the beginnings of a Crypto Trading Dashboard that obtains, loads, and begins to analyze complete trade history for a crypto coin?\n",
    "```\n",
    "As demonstrated, the answer is ***Yes***. \n",
    "\n",
    "**Next steps:**\n",
    "\n",
    "In the future, I can:\n",
    "1. Identify statistics that are most relevant to real-time trade analysis and create code to calculate these statistics, \n",
    "2. Transition from a **'download then analyze'** approach to a **'download and analyze continously'** approach.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
